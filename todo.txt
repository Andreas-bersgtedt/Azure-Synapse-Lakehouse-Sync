1. Add in the databricks notebooks and updated synapse pipelines
2. (Done) Determine the right cluster size/specs
3. (Done) Pick a databricks -> ADLS authentication method
4. (Done) Copy the new dataset to the synapseanalyticspocdata storage account and update the script
5. Create some readme's to accompany the databricks notebooks, synapse pipelines, and example environment template
6. Create a PPT with data lake pattern overview, why this solution, benefits, architecture diagrams
    Focus on 3 scenarios:
        1. Synapse sync notebooks/configuration in customers production
        2. If you use the data lake gold -> synapse pattern already, 'Example Environment' is a quick easy button
           approach to see how this synapse sync works without having to build out or configure in your environment.
        3. If you DO NOT use the data lake gold -> synapse pattern already, 'Example Environment' is a quick easy
           button approach to see how it works.
7. Rename the logging.AutoIngestion Synapse tables to logging.AutoLoader (I did it here but Bret needs to update the dev environment)
8. Change the dedicated sql pool back to 1000dwu
9. Do we want to include sample synapse sql queries to show what's happening/changed?

Synapse Lakehouse Sync Solution
1. (Done) - Add support for using existing delta tables without rewriting with _Id column. Need to add key value column(s) to the metadata. Need to update sql generator to use the key column(s) defined in metadata.
2. Update notebooks to be ran in synapse spark pools. Waiting on functionality of delta alwaysgenerateas identity to be released (ETA H2 2022). If we implement number 1, then we could immediately do this. 
3. (Done) - Add support for adding and removing columns.
4. Look into using the auto_create_table option with the copy into statement. In testing, I found that array types work only when using this feature. Would have to change the auto loader logic to do the max datatypes after the data is staged into the dedicated pool and then do a CTAS with convert statements on all the columns to get the optimial datatype.
5. Add code comments to the notebooks.
