{
	"name": "Synapse - SynapseLakehouseSync_Tutorial",
	"properties": {
		"description": "The pipeline that orchestrates converting parquet files to delta format with change data feed enabled, does a full load of the data, simulates data changes to the source delta tables, run the SynapseLakehouseSync pipeline again after no data changes occurred to verify no new data is added into Synapse.",
		"activities": [
			{
				"name": "Execute - Full Load - SynapseLakehouseSync",
				"description": "Execute the SynapseLakehouseSync pipeline to sync the AdventureWorks delta tables to the Synapse dedicated pool. This will be a full load since the data will not exist in Synapse yet.",
				"type": "ExecutePipeline",
				"dependsOn": [
					{
						"activity": "Spark - Convert Parquet to Delta Tables",
						"dependencyConditions": [
							"Succeeded"
						]
					}
				],
				"userProperties": [],
				"typeProperties": {
					"pipeline": {
						"referenceName": "Synapse - SynapseLakehouseSync",
						"type": "PipelineReference"
					},
					"waitOnCompletion": true,
					"parameters": {
						"StorageAccountNameMetadata": {
							"value": "@concat('https://'\n,pipeline().DataFactory\n,'.dfs.core.windows.net/synapsesync/Synapse_Lakehouse_Sync_Metadata.csv')",
							"type": "Expression"
						}
					}
				}
			},
			{
				"name": "Execute - Changes - SynapseLakehouseSync",
				"description": "Execute the SynapseLakehouseSync pipeline to sync the AdventureWorks delta tables to the Synapse dedicated pool. This will be an incremental load capturing the data changes that occurred in the previous step.",
				"type": "ExecutePipeline",
				"dependsOn": [
					{
						"activity": "Spark - Simulate Data Changes",
						"dependencyConditions": [
							"Succeeded"
						]
					}
				],
				"userProperties": [],
				"typeProperties": {
					"pipeline": {
						"referenceName": "Synapse - SynapseLakehouseSync",
						"type": "PipelineReference"
					},
					"waitOnCompletion": true,
					"parameters": {
						"StorageAccountNameMetadata": {
							"value": "@concat('https://'\n,pipeline().DataFactory\n,'.dfs.core.windows.net/synapsesync/Synapse_Lakehouse_Sync_Metadata.csv')",
							"type": "Expression"
						}
					}
				}
			},
			{
				"name": "Check table row counts for both loads",
				"description": "Query the Synapse logging table to get the row counts from the delta tables and the table in Synapse for each sync process.",
				"type": "Lookup",
				"dependsOn": [
					{
						"activity": "Execute - Changes - SynapseLakehouseSync",
						"dependencyConditions": [
							"Succeeded"
						]
					}
				],
				"policy": {
					"timeout": "0.12:00:00",
					"retry": 0,
					"retryIntervalInSeconds": 30,
					"secureOutput": false,
					"secureInput": false
				},
				"userProperties": [],
				"typeProperties": {
					"source": {
						"type": "SqlDWSource",
						"sqlReaderQuery": "SELECT *, ABS(TableRowCountADLS - TableRowCountSynapse) AS Diff\nFROM logging.SynapseLakehouseSync\nORDER BY TableName, RowInsertDateTime\n",
						"queryTimeout": "24:00:00",
						"partitionOption": "None"
					},
					"dataset": {
						"referenceName": "DS_Synapse_Managed_Identity",
						"type": "DatasetReference",
						"parameters": {
							"ServerName": {
								"value": "@concat(pipeline().DataFactory, '.sql.azuresynapse.net')",
								"type": "Expression"
							},
							"DatabaseName": "DataWarehouse"
						}
					},
					"firstRowOnly": false
				}
			},
			{
				"name": "Drop Synapse Tables If Exist DataWarehouse",
				"description": "Drop any tables that exists in the Synapse dedicated pool. This allows for the pipeline to be executed repeatedly with the same results.",
				"type": "Lookup",
				"dependsOn": [],
				"policy": {
					"timeout": "0.12:00:00",
					"retry": 0,
					"retryIntervalInSeconds": 30,
					"secureOutput": false,
					"secureInput": false
				},
				"userProperties": [],
				"typeProperties": {
					"source": {
						"type": "SqlDWSource",
						"sqlReaderQuery": "DECLARE @sql NVARCHAR(MAX)\n\nSELECT @sql = STRING_AGG(CONVERT(NVARCHAR(MAX), CONCAT('IF OBJECT_ID(''', TABLE_SCHEMA, '.', TABLE_NAME, ''', ''U'') IS NOT NULL DROP TABLE ', TABLE_SCHEMA, '.', TABLE_NAME, ';')), ' ')\nFROM INFORMATION_SCHEMA.TABLES\n\n--PRINT (@sql)\nEXEC (@sql);\n\nSELECT 1 AS a",
						"queryTimeout": "24:00:00",
						"partitionOption": "None"
					},
					"dataset": {
						"referenceName": "DS_Synapse_Managed_Identity",
						"type": "DatasetReference",
						"parameters": {
							"ServerName": {
								"value": "@concat(pipeline().DataFactory, '.sql.azuresynapse.net')",
								"type": "Expression"
							},
							"DatabaseName": "DataWarehouse"
						}
					},
					"firstRowOnly": false
				}
			},
			{
				"name": "Wait - 10 seconds",
				"description": "Sleep for 10 seconds between Synapse Lakehouse Sync loads.",
				"type": "Wait",
				"dependsOn": [
					{
						"activity": "Check table row counts for both loads",
						"dependencyConditions": [
							"Succeeded"
						]
					},
					{
						"activity": "Check table row counts for both loads Finance",
						"dependencyConditions": [
							"Succeeded"
						]
					}
				],
				"userProperties": [],
				"typeProperties": {
					"waitTimeInSeconds": 1
				}
			},
			{
				"name": "Execute - Changes - SynapseLakehouseSync_copy1",
				"description": "Execute the SynapseLakehouseSync pipeline to sync the AdventureWorks delta tables to the Synapse dedicated pool. This will not load any data since no data changes have occurred on the source delta tables.",
				"type": "ExecutePipeline",
				"dependsOn": [
					{
						"activity": "Wait - 10 seconds",
						"dependencyConditions": [
							"Succeeded"
						]
					}
				],
				"userProperties": [],
				"typeProperties": {
					"pipeline": {
						"referenceName": "Synapse - SynapseLakehouseSync",
						"type": "PipelineReference"
					},
					"waitOnCompletion": true,
					"parameters": {
						"StorageAccountNameMetadata": {
							"value": "@concat('https://'\n,pipeline().DataFactory\n,'.dfs.core.windows.net/synapsesync/Synapse_Lakehouse_Sync_Metadata.csv')",
							"type": "Expression"
						}
					}
				}
			},
			{
				"name": "Check table row counts - No Data Changes",
				"description": "Query the Synapse logging table to get the row counts from the delta tables and the table in Synapse for each sync process. This should match the original row counts from before since no data changes should have occurred this third run.",
				"type": "Lookup",
				"dependsOn": [
					{
						"activity": "Execute - Changes - SynapseLakehouseSync_copy1",
						"dependencyConditions": [
							"Succeeded"
						]
					}
				],
				"policy": {
					"timeout": "0.12:00:00",
					"retry": 0,
					"retryIntervalInSeconds": 30,
					"secureOutput": false,
					"secureInput": false
				},
				"userProperties": [],
				"typeProperties": {
					"source": {
						"type": "SqlDWSource",
						"sqlReaderQuery": "SELECT *, ABS(TableRowCountADLS - TableRowCountSynapse) AS Diff\nFROM logging.SynapseLakehouseSync\nORDER BY TableName, RowInsertDateTime\n",
						"queryTimeout": "24:00:00",
						"partitionOption": "None"
					},
					"dataset": {
						"referenceName": "DS_Synapse_Managed_Identity",
						"type": "DatasetReference",
						"parameters": {
							"ServerName": {
								"value": "@concat(pipeline().DataFactory, '.sql.azuresynapse.net')",
								"type": "Expression"
							},
							"DatabaseName": "DataWarehouse"
						}
					},
					"firstRowOnly": false
				}
			},
			{
				"name": "Drop Synapse Tables If Exist DataWarehouse_Finance",
				"description": "Drop any tables that exists in the Synapse dedicated pool. This allows for the pipeline to be executed repeatedly with the same results.",
				"type": "Lookup",
				"dependsOn": [],
				"policy": {
					"timeout": "0.12:00:00",
					"retry": 0,
					"retryIntervalInSeconds": 30,
					"secureOutput": false,
					"secureInput": false
				},
				"userProperties": [],
				"typeProperties": {
					"source": {
						"type": "SqlDWSource",
						"sqlReaderQuery": "DECLARE @sql NVARCHAR(MAX)\n\nSELECT @sql = STRING_AGG(CONVERT(NVARCHAR(MAX), CONCAT('IF OBJECT_ID(''', TABLE_SCHEMA, '.', TABLE_NAME, ''', ''U'') IS NOT NULL DROP TABLE ', TABLE_SCHEMA, '.', TABLE_NAME, ';')), ' ')\nFROM INFORMATION_SCHEMA.TABLES\n\n--PRINT (@sql)\nEXEC (@sql);\n\nSELECT 1 AS a",
						"queryTimeout": "24:00:00",
						"partitionOption": "None"
					},
					"dataset": {
						"referenceName": "DS_Synapse_Managed_Identity",
						"type": "DatasetReference",
						"parameters": {
							"ServerName": {
								"value": "@concat(pipeline().DataFactory, '.sql.azuresynapse.net')",
								"type": "Expression"
							},
							"DatabaseName": "DataWarehouse_Finance"
						}
					},
					"firstRowOnly": false
				}
			},
			{
				"name": "Check table row counts for both loads Finance",
				"description": "Query the Synapse logging table to get the row counts from the delta tables and the table in Synapse for each sync process.",
				"type": "Lookup",
				"dependsOn": [
					{
						"activity": "Execute - Changes - SynapseLakehouseSync",
						"dependencyConditions": [
							"Succeeded"
						]
					}
				],
				"policy": {
					"timeout": "0.12:00:00",
					"retry": 0,
					"retryIntervalInSeconds": 30,
					"secureOutput": false,
					"secureInput": false
				},
				"userProperties": [],
				"typeProperties": {
					"source": {
						"type": "SqlDWSource",
						"sqlReaderQuery": "SELECT *, ABS(TableRowCountADLS - TableRowCountSynapse) AS Diff\nFROM logging.SynapseLakehouseSync\nORDER BY TableName, RowInsertDateTime\n",
						"queryTimeout": "24:00:00",
						"partitionOption": "None"
					},
					"dataset": {
						"referenceName": "DS_Synapse_Managed_Identity",
						"type": "DatasetReference",
						"parameters": {
							"ServerName": {
								"value": "@concat(pipeline().DataFactory, '.sql.azuresynapse.net')",
								"type": "Expression"
							},
							"DatabaseName": "DataWarehouse_Finance"
						}
					},
					"firstRowOnly": false
				}
			},
			{
				"name": "Check table row counts - No Data Changes Finance",
				"description": "Query the Synapse logging table to get the row counts from the delta tables and the table in Synapse for each sync process. This should match the original row counts from before since no data changes should have occurred this third run.",
				"type": "Lookup",
				"dependsOn": [
					{
						"activity": "Execute - Changes - SynapseLakehouseSync_copy1",
						"dependencyConditions": [
							"Succeeded"
						]
					}
				],
				"policy": {
					"timeout": "0.12:00:00",
					"retry": 0,
					"retryIntervalInSeconds": 30,
					"secureOutput": false,
					"secureInput": false
				},
				"userProperties": [],
				"typeProperties": {
					"source": {
						"type": "SqlDWSource",
						"sqlReaderQuery": "SELECT *, ABS(TableRowCountADLS - TableRowCountSynapse) AS Diff\nFROM logging.SynapseLakehouseSync\nORDER BY TableName, RowInsertDateTime\n",
						"queryTimeout": "24:00:00",
						"partitionOption": "None"
					},
					"dataset": {
						"referenceName": "DS_Synapse_Managed_Identity",
						"type": "DatasetReference",
						"parameters": {
							"ServerName": {
								"value": "@concat(pipeline().DataFactory, '.sql.azuresynapse.net')",
								"type": "Expression"
							},
							"DatabaseName": "DataWarehouse_Finance"
						}
					},
					"firstRowOnly": false
				}
			},
			{
				"name": "Spark - Convert Parquet to Delta Tables",
				"description": "Run an Azure Databricks notebook that will convert 6 tables in parquet format to delta 2.0 format with change data feed enabled and adding an _Id column.",
				"type": "SynapseNotebook",
				"dependsOn": [
					{
						"activity": "Drop Synapse Tables If Exist DataWarehouse",
						"dependencyConditions": [
							"Succeeded"
						]
					},
					{
						"activity": "Drop Synapse Tables If Exist DataWarehouse_Finance",
						"dependencyConditions": [
							"Succeeded"
						]
					}
				],
				"policy": {
					"timeout": "7.00:00:00",
					"retry": 0,
					"retryIntervalInSeconds": 30,
					"secureOutput": false,
					"secureInput": false
				},
				"userProperties": [],
				"typeProperties": {
					"notebook": {
						"referenceName": "Convert Parquet to Delta Tables - AdventureWorks",
						"type": "NotebookReference"
					},
					"parameters": {
						"SynapseLakehouseSyncParameters": {
							"value": {
								"value": "@concat('{\"DatabaseName\":\"AdventureWorks\", \"SynapseWorkspaceName\":\"', pipeline().DataFactory, '\"\n, \"ParquetDataADLSFullPath\":\"abfss://gold@enterprisedatalake', string(split(pipeline().DataFactory, 'sync')[1]), '.dfs.core.windows.net/Sample/\"\n}')",
								"type": "Expression"
							},
							"type": "string"
						}
					},
					"snapshot": true,
					"sparkPool": {
						"referenceName": "SyncCluster",
						"type": "BigDataPoolReference"
					}
				}
			},
			{
				"name": "Spark - Simulate Data Changes",
				"description": "Run an Azure Databricks notebook that simulates data changes (deletes, updates, and inserts) to the AdventureWorks delta tables. ",
				"type": "SynapseNotebook",
				"dependsOn": [
					{
						"activity": "Execute - Full Load - SynapseLakehouseSync",
						"dependencyConditions": [
							"Succeeded"
						]
					}
				],
				"policy": {
					"timeout": "7.00:00:00",
					"retry": 0,
					"retryIntervalInSeconds": 30,
					"secureOutput": false,
					"secureInput": false
				},
				"userProperties": [],
				"typeProperties": {
					"notebook": {
						"referenceName": "Simulate Data Changes - AdventureWorks",
						"type": "NotebookReference"
					},
					"parameters": {
						"SynapseLakehouseSyncParameters": {
							"value": {
								"value": "@concat('{\"DatabaseName\":\"AdventureWorks\", \"SynapseWorkspaceName\":\"', pipeline().DataFactory, '\"\n, \"ParquetDataADLSFullPath\":\"abfss://gold@enterprisedatalake', string(split(pipeline().DataFactory, 'sync')[1]), '.dfs.core.windows.net/Sample/\"\n, \"ParquetDataDatabricksKeyVaultScope\":\"AzureKeyVaultScope\"\n, \"ParquetDataAzureKeyVaultSecretName\":\"EnterpriseDataLakeAccountKey\"}'\n)",
								"type": "Expression"
							},
							"type": "string"
						}
					},
					"snapshot": true,
					"sparkPool": {
						"referenceName": "SyncCluster",
						"type": "BigDataPoolReference"
					}
				}
			}
		],
		"variables": {
			"ParquetToDeltaSuccessFlag": {
				"type": "Boolean",
				"defaultValue": false
			}
		},
		"folder": {
			"name": "Synapse - Synapse Lakehouse Sync Tutorial"
		},
		"annotations": [],
		"lastPublishTime": "2023-02-05T20:05:08Z"
	},
	"type": "Microsoft.Synapse/workspaces/pipelines"
}