{
	"name": "Synapse Lakehouse Sync ADLS",
	"properties": {
		"folder": {
			"name": "Synapse Lakehouse Sync"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "SyncCluster",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 1,
			"runAsWorkspaceSystemIdentity": true,
			"conf": {
				"spark.dynamicAllocation.enabled": "true",
				"spark.dynamicAllocation.minExecutors": "1",
				"spark.dynamicAllocation.maxExecutors": "4",
				"spark.autotune.trackingId": "7574a0b5-b1f7-41e1-ba44-87f502dbf358"
			}
		},
		"metadata": {
			"saveOutput": true,
			"synapse_widget": {
				"version": "0.1"
			},
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/18bc4af3-8099-4e70-86d8-eba06dd5bac8/resourceGroups/Azure-Synapse-Lakehouse-Sync/providers/Microsoft.Synapse/workspaces/synapsesyncqbq/bigDataPools/SyncCluster",
				"name": "SyncCluster",
				"type": "Spark",
				"endpoint": "https://synapsesyncqbq.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SyncCluster",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net",
					"authHeader": null
				},
				"sparkVersion": "3.3",
				"nodeCount": 5,
				"cores": 4,
				"memory": 28,
				"extraHeader": null
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"application/vnd.databricks.v1+cell": {
						"showTitle": false,
						"nuid": "3571454b-8abb-49e6-8bf3-e2164d93c54f",
						"title": ""
					}
				},
				"source": [
					"### Synapse Lakehouse Sync ADLS\n",
					"\n",
					"This notebook will produce the changed data in parquet format partitioned by the changed type. This is data that the pipeline was later stage into Synapse to load to the final destination tables. \n",
					"\n",
					"0. Standardizes the parameter folder paths \n",
					"0. Creates a _SynapseLakehouseSync tracking delta table for the specified pool if one does not already exist\n",
					"0. Determine what load type is required by checking the tracking table and the ExistsFlagSynapse parameter\n",
					"0. Write out the tables changes either full_load or incremental with one more types (delete, update, insert)\n",
					"0. Create an entry in the _SynapseLakehouseSync tracking delta table for the changed data that was written\n",
					"0. Output the json of the change types and standardized folder paths to be used later in the pipeline\n",
					"\n",
					"#### Setup Requirements\n",
					"The tables that are going to be sync'd must have change data feed enabled. https://docs.delta.io/latest/delta-change-data-feed.html\n",
					"```\n",
					"ALTER TABLE myDeltaTable SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\n",
					"```\n",
					"\n",
					"#### Notebook Parameters\n",
					"- **DataFolderPathFull** - The full path in either abfss or https format to the delta table\n",
					"- **ExistsFlagSynapse** - A flag (True or False) if the table was found in the Synapse Dedicated Pool\n",
					"- **KeyColumns** - The comma separate list of columns that make a row unique in the table. If you followed the <a href=\"$../Synapse Lakehouse Sync Tutorial/Convert Parquet to Delta Tables - AdventureWorks\">Convert Parquet to Delta Table - Adventures</a> notebook, an _Id column (BIGINT GENERATED ALWAYS AS IDENTITY) is added to the delta table. This extra column simplifies and improves performance the sync process. If you do not want to add the extra _Id, you pass in the one or more columns in a comma seperated format. Example, SalesOrderNumber,SalesOrderLineNumber\n",
					"- **PoolName** - The Synapse Dedicated Pool name that the data will be loaded into\n",
					"- **SchemaName** - The schema name of the table to be loaded in the Synapse Dedicated Pool\n",
					"- **DataLakeName** - The ADLS storage account name where the delta data exists\n",
					"- **SyncFolderPathFull** - The full path in either abfss or https format that the CDC changes will be loaded to as well as where the _SynapseLakehouseSync delta table is stored\n",
					"- **TableName** - The table name that is being sync'd"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"tags": [
						"parameters"
					]
				},
				"source": [
					"SynapseLakehouseSyncParameters = '''\r\n",
					"{\r\n",
					"    \"ExistsFlagSynapse\": \"True\",\r\n",
					"    \"SynapseWorkspaceName\": \"synapsesyncqbq\",\r\n",
					"    \"PoolName\": \"DataWarehouse\",\r\n",
					"    \"SchemaName\": \"AdventureWorks\",\r\n",
					"    \"TableName\": \"FactResellerSales\",\r\n",
					"    \"KeyColumns\": \"SalesOrderNumber,SalesOrderLineNumber\",\r\n",
					"    \"DeltaDataADLSFullPath\": \"abfss://gold@enterprisedatalakeqbq.dfs.core.windows.net/Sample/AdventureWorks/FactResellerSales\",\r\n",
					"    \"SynapseSyncDataADLSFullPath\": \"abfss://gold@enterprisedatalakeqbq.dfs.core.windows.net/Sample/\",\r\n",
					"    \"DropTableFlag\": \"True\"\r\n",
					"}\r\n",
					"'''"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"application/vnd.databricks.v1+cell": {
						"showTitle": true,
						"nuid": "c8881b59-b5d6-466c-9c05-c889d6b825bd",
						"title": "Parameters"
					},
					"tags": []
				},
				"source": [
					"import json\n",
					"\n",
					"SynapseLakehouseSyncParameters = json.loads(SynapseLakehouseSyncParameters)\n",
					"\n",
					"print(json.dumps(SynapseLakehouseSyncParameters, indent=4))"
				],
				"execution_count": 5
			},
			{
				"cell_type": "code",
				"metadata": {
					"application/vnd.databricks.v1+cell": {
						"showTitle": true,
						"nuid": "f1d3a00d-9e3c-465d-80fe-d2513c0356f7",
						"title": "Import Functions"
					}
				},
				"source": [
					"%run \"/Synapse Lakehouse Sync/Synapse Lakehouse Sync Functions\""
				],
				"execution_count": 6
			},
			{
				"cell_type": "code",
				"metadata": {
					"application/vnd.databricks.v1+cell": {
						"showTitle": true,
						"nuid": "612f1362-17bd-4918-9e3e-98ae1f54bca5",
						"title": "Standardize Folder Paths"
					}
				},
				"source": [
					"DataFolderPathDict = UnifyFolderPaths('SynapseSync', SynapseLakehouseSyncParameters[\"DeltaDataADLSFullPath\"])\n",
					"SyncFolderPathDict = UnifyFolderPaths('SynapseSync', f'{SynapseLakehouseSyncParameters[\"SynapseSyncDataADLSFullPath\"].rstrip(\"/\")}')\n",
					"\n",
					"print(DataFolderPathDict)\n",
					"print(SyncFolderPathDict)"
				],
				"execution_count": 7
			},
			{
				"cell_type": "code",
				"metadata": {
					"application/vnd.databricks.v1+cell": {
						"showTitle": true,
						"nuid": "5aac8e21-4edf-49c2-86ef-b992f31e05da",
						"title": "Instantiate the _SynapseLakehouseSync Tracker Table"
					}
				},
				"source": [
					"track = Tracker(SynapseWorkspaceName=SynapseLakehouseSyncParameters[\"SynapseWorkspaceName\"], PoolName=SynapseLakehouseSyncParameters[\"PoolName\"], TrackerFolderPath=SyncFolderPathDict[\"SynapseSync_abfss\"])"
				],
				"execution_count": 8
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"application/vnd.databricks.v1+cell": {
						"showTitle": false,
						"nuid": "3e9898d3-0dfd-4ad5-8388-df5585500895",
						"title": ""
					}
				},
				"source": [
					"#### Writing of the data changes logic\n",
					"1. Get the latest version from the 'history' of the table.\n",
					"2. Get the last successful load entry from the _SynapseLakehouseSync table.\n",
					"3. Write the data changes in parquet format to ADLS\n",
					"   * If the ExistsFlagSynapse parameter is true and a successful load was found in the _SynapseLakehouseSync table, then do an incremental load. If the version end of last successful entry from the tracking table matches the latest version from the 'history' table, then no data is written since changes have not occurred on the table since the last sync execution.\n",
					"   * If the ExistsFlagSynapse parameter is false or a successful load was not found in the _SynapseLakehouse Table, a full load occurs writing out all the data from the latest version of the table."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"application/vnd.databricks.v1+cell": {
						"showTitle": true,
						"nuid": "2f3a5626-6645-489f-9a80-d7ec96510f5f",
						"title": "Write changes to ADLS"
					}
				},
				"source": [
					"import datetime\n",
					"from pyspark.sql.functions import col, max\n",
					"import pyspark\n",
					"from delta import *\n",
					"\n",
					"tracker = track.GetLastSuccessfulSynapseLoad(SynapseLakehouseSyncParameters[\"SchemaName\"], SynapseLakehouseSyncParameters[\"TableName\"]).collect()\n",
					"\n",
					"dfHistoryVersionMax = DeltaTable.forPath(spark, f\"{DataFolderPathDict['SynapseSync_abfss']}\").history().select(max(col('version')).alias(\"versionMax\"), max(col('timestamp')).alias('timestampMax') )\n",
					"\n",
					"historyVersionMax = dfHistoryVersionMax.collect()\n",
					"\n",
					"#Remove historical synapse sync files\n",
					"try:\n",
					"  #Delete the change folder if it exists for the table\n",
					"  mssparkutils.fs.rm(f'{SyncFolderPathDict[\"SynapseSync_abfss\"]}/{SynapseLakehouseSyncParameters[\"SynapseWorkspaceName\"]}_{SynapseLakehouseSyncParameters[\"PoolName\"]}_SynapseLakehouseSync/{SynapseLakehouseSyncParameters[\"SchemaName\"]}/{SynapseLakehouseSyncParameters[\"TableName\"]}', True)\n",
					"except:\n",
					"  print('Location does not already exist - {SyncFolderPathDict[\"SynapseSync_abfss\"]}/{SynapseLakehouseSyncParameters[\"SynapseWorkspaceName\"]}_{SynapseLakehouseSyncParameters[\"PoolName\"]}_SynapseLakehouseSync/{SynapseLakehouseSyncParameters[\"SchemaName\"]}/{SynapseLakehouseSyncParameters[\"TableName\"]}')\n",
					"\n",
					"LoadType = 'No Changes Found' #default loadtype for the sync table\n",
					"TableRowCountADLS = None\n",
					"\n",
					"\"\"\"\n",
					"Determine whether a sync process can occur and if it can, what load type should occur.\n",
					"Logic:\n",
					"  1. If no versions are found for the delta table, we cannot do a sync. \n",
					"  2. If version are found for the delta table and the table exists in the Synapse dedicated pool and we have data in the Synase Lakehouse tracking table and the last version record from the tracking table is less than the max version from the history of the table, then do an incrential load.\n",
					"  3. If version are found for the delta table and the table does NOT exist in Synapse or no sync process data has been recorded in the Synapse lakehouse tracking table, then do a full load (will drop the table in Synapse if it exists. This is required because the sync tracking data is off and would risk haivng the different data in Synapse and the datalake.)\n",
					"\"\"\"\n",
					"if len(historyVersionMax) > 0:\n",
					"  if eval(SynapseLakehouseSyncParameters['ExistsFlagSynapse']) and len(tracker) > 0:\n",
					"    versionMin = (tracker[0].VersionNumberEnd) + 1 #Get the VersionEndNumber and add one since change feed functionality is inclusive on the start and end versions.\n",
					"    versionMax = historyVersionMax[0].versionMax #Get the max version from the history table\n",
					"    timestampMin = tracker[0].DateTimeEnd #Get the DateTimeEnd\n",
					"    timestampMax = historyVersionMax[0].timestampMax\n",
					"\n",
					"    print(f'{SynapseLakehouseSyncParameters[\"SchemaName\"]}.{SynapseLakehouseSyncParameters[\"TableName\"]} - versionMin: {versionMin}, versionMax: {versionMax}')\n",
					"    if versionMin < versionMax:\n",
					"      LoadType = 'Incremental'\n",
					"      \n",
					"    else:\n",
					"      print(f\"The versionMin '{versionMin}' was larger or equal to the versionMax '{versionMax}' for table '{SynapseLakehouseSyncParameters['SchemaName']}.{SynapseLakehouseSyncParameters['TableName']}'\")\n",
					"  else:\n",
					"    LoadType = 'Full'\n",
					"\n",
					"    \n",
					"\"\"\"\n",
					"If the table exists in Synapse and we have tracking data on the sync to synapse, do an incremental load\n",
					"Else do a full load\n",
					"\"\"\"\n",
					"if LoadType == 'Incremental':\n",
					"  print(f'{SynapseLakehouseSyncParameters[\"SchemaName\"]}.{SynapseLakehouseSyncParameters[\"TableName\"]} - Incremental Load')\n",
					"\n",
					"  df = spark.read.format(\"delta\").option(\"versionAsOf\", versionMax).load(f\"{DataFolderPathDict['SynapseSync_abfss']}\")\n",
					"  TableRowCountADLS = df.count()\n",
					"  print(f'Row Count - {TableRowCountADLS}')\n",
					"  \n",
					"  df = (spark.read.format(\"delta\") \n",
					"    .option(\"readChangeFeed\", \"true\") \n",
					"    .option(\"startingVersion\", versionMin)\n",
					"    .option(\"endingVersion\", versionMax)\n",
					"    .load(f\"{DataFolderPathDict['SynapseSync_abfss']}\")\n",
					"   )\n",
					"\n",
					"  print(f'\\tTotal Changed Row Count - {df.count()}')\n",
					"  \n",
					"  df.createOrReplaceTempView(\"vwChangeDataFeed\")\n",
					"\n",
					"  dfMostRecentRecord = spark.sql(f\"\"\"\n",
					"    --Logic to identify the most recent changes to a change and swapt\n",
					"    --Get the first record and the first value per key column and compare with the last value\n",
					"    SELECT  *\n",
					"            ,CASE WHEN _FirstValue = 'insert' AND _change_type = 'update_postimage' THEN 'insert'\n",
					"              WHEN _FirstValue = 'delete' AND _change_type = 'insert' THEN 'update_postimage'\n",
					"              ELSE _change_type\n",
					"              END AS _change_type_altered\n",
					"    FROM\n",
					"    (\n",
					"      SELECT  *\n",
					"              ,FIRST_VALUE(_change_type) OVER (PARTITION BY {SynapseLakehouseSyncParameters['KeyColumns']} ORDER BY _commit_timestamp ASC) AS _FirstValue\n",
					"              ,ROW_NUMBER() OVER (PARTITION BY {SynapseLakehouseSyncParameters['KeyColumns']} ORDER BY _commit_timestamp DESC) AS _RN\n",
					"      FROM vwChangeDataFeed\n",
					"      WHERE _change_type != 'update_preimage'\n",
					"    )\n",
					"    WHERE _RN = 1\n",
					"  \"\"\")\n",
					"\n",
					"  dfFiltered = dfMostRecentRecord.drop('_RN', '_FirstValue', '_change_type', '_commit_version', '_commit_timestamp').withColumnRenamed('_change_type_altered', '_change_type')\n",
					"  dfFiltered.write.partitionBy('_change_type').mode('overwrite').parquet(f'{SyncFolderPathDict[\"SynapseSync_abfss\"]}/{SynapseLakehouseSyncParameters[\"SynapseWorkspaceName\"]}_{SynapseLakehouseSyncParameters[\"PoolName\"]}_SynapseLakehouseSync/{SynapseLakehouseSyncParameters[\"SchemaName\"]}/{SynapseLakehouseSyncParameters[\"TableName\"]}')\n",
					"\n",
					"elif LoadType == 'Full':\n",
					"  print(f'{SynapseLakehouseSyncParameters[\"SchemaName\"]}.{SynapseLakehouseSyncParameters[\"TableName\"]} - Full Load')\n",
					"  \n",
					"  versionMin = historyVersionMax[0].versionMax #Assigning the version min the same as max for full loads\n",
					"  versionMax = historyVersionMax[0].versionMax\n",
					"  timestampMin = historyVersionMax[0].timestampMax #Assigning the DateTimeStart the same as end for full loads\n",
					"  timestampMax = historyVersionMax[0].timestampMax\n",
					"  \n",
					"  print(f'Full Load - {SynapseLakehouseSyncParameters[\"SchemaName\"]}.{SynapseLakehouseSyncParameters[\"TableName\"]} - versionMin: {versionMin}, versionMax: {versionMax}')\n",
					"\n",
					"  df = spark.read.format(\"delta\").option(\"versionAsOf\", versionMax).load(f\"{DataFolderPathDict['SynapseSync_abfss']}\")\n",
					"  TableRowCountADLS = df.count()\n",
					"  \n",
					"  print(f'Row Count - {TableRowCountADLS}')\n",
					"\n",
					"  df.write.mode('overwrite').parquet(f'{SyncFolderPathDict[\"SynapseSync_abfss\"]}/{SynapseLakehouseSyncParameters[\"SynapseWorkspaceName\"]}_{SynapseLakehouseSyncParameters[\"PoolName\"]}_SynapseLakehouseSync/{SynapseLakehouseSyncParameters[\"SchemaName\"]}/{SynapseLakehouseSyncParameters[\"TableName\"]}/_change_type=full_load')\n",
					"else:\n",
					"  print(f\"No versions were found for table '{SynapseLakehouseSyncParameters['SchemaName']}.{SynapseLakehouseSyncParameters['TableName']}' in history describe.\\nCannot sync data for table '{SynapseLakehouseSyncParameters['SchemaName']}.{SynapseLakehouseSyncParameters['TableName']}'.\")\n",
					""
				],
				"execution_count": 19
			},
			{
				"cell_type": "code",
				"metadata": {
					"application/vnd.databricks.v1+cell": {
						"showTitle": true,
						"nuid": "1dc15761-9afd-4c77-9566-0fbffd01f486",
						"title": "Add the types of changes to the dictionary for outputting to the pipeline"
					}
				},
				"source": [
					"folderList = list()\n",
					"if LoadType != 'No Changes Found':\n",
					"  for folder in mssparkutils.fs.ls(f'{SyncFolderPathDict[\"SynapseSync_abfss\"]}/{SynapseLakehouseSyncParameters[\"SynapseWorkspaceName\"]}_{SynapseLakehouseSyncParameters[\"PoolName\"]}_SynapseLakehouseSync/{SynapseLakehouseSyncParameters[\"SchemaName\"]}/{SynapseLakehouseSyncParameters[\"TableName\"]}'):\n",
					"    changeType = folder.name.replace('_change_type=', '').replace('/', '')\n",
					"    if changeType in ['delete', 'insert', 'update_postimage', 'full_load']:\n",
					"      folderList.append(changeType)\n",
					"SyncFolderPathDict.update({'ChangeTypes':folderList, 'TableRowCountADLS':TableRowCountADLS})\n",
					"SyncFolderPathDict.update(SynapseLakehouseSyncParameters)\n",
					"print(SyncFolderPathDict)"
				],
				"execution_count": 21
			},
			{
				"cell_type": "code",
				"metadata": {
					"application/vnd.databricks.v1+cell": {
						"showTitle": true,
						"nuid": "20c0f1b9-2680-4929-b9ee-c6ce622bf798",
						"title": "Log the data export"
					}
				},
				"source": [
					"SynapseLakehouseSyncKey = track.AddEntry(SchemaName=SynapseLakehouseSyncParameters[\"SchemaName\"], TableName=SynapseLakehouseSyncParameters[\"TableName\"], VersionStart=versionMin, VersionEnd=versionMax, DateTimeStart=timestampMin, DateTimeEnd=timestampMax, LoadType=LoadType, ChangeTypes=SyncFolderPathDict['ChangeTypes'], TableRowCountADLS=TableRowCountADLS)\n",
					"\n",
					"SyncFolderPathDict['SynapseLakehouseSyncKey'] = SynapseLakehouseSyncKey\n",
					"\n",
					"print(SyncFolderPathDict)"
				],
				"execution_count": 22
			},
			{
				"cell_type": "code",
				"metadata": {
					"application/vnd.databricks.v1+cell": {
						"showTitle": true,
						"nuid": "f356dfd7-4549-447e-86f6-de85540c8726",
						"title": "Output the unified storage paths to the pipeline"
					}
				},
				"source": [
					"import json\n",
					"mssparkutils.notebook.exit(json.dumps(SyncFolderPathDict))"
				],
				"execution_count": 23
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					""
				]
			}
		]
	}
}