{
	"name": "Convert Parquet to Delta Tables - AdventureWorks",
	"properties": {
		"folder": {
			"name": "Synapse Lakehouse Sync Tutorial"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "SyncCluster",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 1,
			"runAsWorkspaceSystemIdentity": true,
			"conf": {
				"spark.dynamicAllocation.enabled": "true",
				"spark.dynamicAllocation.minExecutors": "1",
				"spark.dynamicAllocation.maxExecutors": "4",
				"spark.autotune.trackingId": "48e0076e-8597-46ff-a436-9f99f52ae359"
			}
		},
		"metadata": {
			"saveOutput": true,
			"synapse_widget": {
				"version": "0.1"
			},
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/18bc4af3-8099-4e70-86d8-eba06dd5bac8/resourceGroups/Azure-Synapse-Lakehouse-Sync/providers/Microsoft.Synapse/workspaces/synapsesyncqbq/bigDataPools/SyncCluster",
				"name": "SyncCluster",
				"type": "Spark",
				"endpoint": "https://synapsesyncqbq.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SyncCluster",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net",
					"authHeader": null
				},
				"sparkVersion": "3.3",
				"nodeCount": 5,
				"cores": 4,
				"memory": 28,
				"extraHeader": null
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"application/vnd.databricks.v1+cell": {
						"showTitle": false,
						"nuid": "901460bd-9884-47c4-8af3-e66cca0d20d0",
						"title": ""
					}
				},
				"source": [
					"### Convert Parquet to Delta Tables - AdventureWorks\n",
					"\n",
					"This is the first Databricks notebook of two in the Azure Synapse Lakehouse Sync tutorial. It demonstrates taking several standard parquet tables and converting them to Delta 2.0, which should be considered our Gold Zone. The Delta 2.0 tables are created using the [Change Data Feed](https://docs.delta.io/2.0.0rc1/delta-change-data-feed.html) feature, enabled by setting ```TBLPROPERTIES (delta.enableChangeDataFeed = true)``` at the table level. Change Data Feed must be enabled for all tables that are synchronized to Synapse Dedicated SQL.\n",
					"\n",
					"In addition, an **_Id** identity column is added to each table with the following syntax ```BIGINT GENERATED ALWAYS AS IDENTITY (START WITH 1 INCREMENT BY 1)```. While not technically required, adding the **_Id** identity column makes the synchronization process to Synapse Dedicated SQL simpiler and faster.\n",
					"\n",
					"For the purposes of the tutorial this notebook is executed by the **SynapseLakehouseSync_Tutorial** pipeline in the Synapse Analytics Workspace."
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"application/vnd.databricks.v1+cell": {
						"showTitle": false,
						"nuid": "a85f20e3-6d9e-4a2c-bf53-1e3b75e5d45e",
						"title": ""
					}
				},
				"source": [
					"### Notebook Parameters\n",
					"\n",
					"The below parameters are populated at runtime by the **SynapseLakehouseSync_Tutorial** pipeline in the Synapse Analytics Workspace.\n",
					"\n",
					"<br>\n",
					"\n",
					"- **DeltaDataFolderPathFull**: The Azure Data Lake Storage path which contains our sample parquet tables and will contain the Delta 2.0 Gold Zone tables\n",
					"- **DatabaseName**: The Delta 2.0 database name and the Synapse Dedicated SQL database to be synchronized to\n",
					"- **DataLakeName**: The ADLS storage account name where the AdventureWorks parquet data exists"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"tags": [
						"parameters"
					]
				},
				"source": [
					"SynapseLakehouseSyncParameters = '''\r\n",
					"{\r\n",
					"    \"DatabaseName\": \"AdventureWorks\",\r\n",
					"    \"SynapseWorkspaceName\": \"synapsesyncqbq\",\r\n",
					"    \"ParquetDataADLSFullPath\": \"abfss://gold@enterprisedatalakeqbq.dfs.core.windows.net/Sample/\"\r\n",
					"}\r\n",
					"'''"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"application/vnd.databricks.v1+cell": {
						"showTitle": false,
						"nuid": "591d1d2d-237c-482d-b966-47e57a4ba6a5",
						"title": ""
					},
					"tags": []
				},
				"source": [
					"import json\n",
					"\n",
					"SynapseLakehouseSyncParameters = json.loads(SynapseLakehouseSyncParameters)\n",
					"\n",
					"print(json.dumps(SynapseLakehouseSyncParameters, indent=4))"
				],
				"execution_count": 2
			},
			{
				"cell_type": "code",
				"metadata": {
					"application/vnd.databricks.v1+cell": {
						"showTitle": true,
						"nuid": "564d88be-6e2b-4be7-9a2c-74fd7353de1c",
						"title": "Import Functions"
					}
				},
				"source": [
					"%run \"/Synapse Lakehouse Sync/Synapse Lakehouse Sync Functions\""
				],
				"execution_count": 3
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"application/vnd.databricks.v1+cell": {
						"showTitle": false,
						"nuid": "3d307f65-7725-40c6-a30c-5126b56afe84",
						"title": ""
					}
				},
				"source": [
					"### Convert Parquet to Delta 2.0 Gold Zone Tables\n",
					"\n",
					"This converts our sample parquet tables to Delta 2.0 tables and adds an **_Id** identity column. Delta 2.0 and above supports [Change Data Feed](https://docs.delta.io/2.0.0rc1/delta-change-data-feed.html) and is the foundation of Azure Synapse Lakehouse Sync. The **_Id** identity column is added to ensure a unique column exists which helps in identifying and updating record changes."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"application/vnd.databricks.v1+cell": {
						"showTitle": false,
						"nuid": "dc91173a-1d32-44d3-aa2d-15a72915e23d",
						"title": ""
					}
				},
				"source": [
					"import pyspark\n",
					"from delta import *\n",
					"\n",
					"# Drop and delete the AdventureWorks Delta database and files if they already exist\n",
					"spark.sql(f\"DROP DATABASE IF EXISTS {SynapseLakehouseSyncParameters['DatabaseName']} CASCADE\")\n",
					"\n",
					"try:\n",
					"    #Remove the AdventureWorks folder if it exists. This is here for repeatability of the tutorial.\n",
					"    mssparkutils.fs.rm(f'{SynapseLakehouseSyncParameters[\"ParquetDataADLSFullPath\"].rstrip(\"/\")}/AdventureWorks', True)\n",
					"except:\n",
					"    print(f\"Location didnt already exist - {SynapseLakehouseSyncParameters['ParquetDataADLSFullPath'].rstrip('/')}/AdventureWorks\")\n",
					"\n",
					"try:\n",
					"    # Delete the Synapse Lakehouse Sync Tracking Delta files if they exist\n",
					"    mssparkutils.fs.rm(f'{SynapseLakehouseSyncParameters[\"ParquetDataADLSFullPath\"]}/{SynapseLakehouseSyncParameters[\"SynapseWorkspaceName\"]}_DataWarehouse_SynapseLakehouseSync', True)\n",
					"except:\n",
					"    print(f\"Location didn't already exist - {SynapseLakehouseSyncParameters['ParquetDataADLSFullPath']}/{SynapseLakehouseSyncParameters['SynapseWorkspaceName']}_DataWarehouse_SynapseLakehouseSync\")\n",
					"\n",
					"# Create a new AdventureWorks (Delta) database and tables\n",
					"spark.sql(f\"CREATE DATABASE IF NOT EXISTS {SynapseLakehouseSyncParameters['DatabaseName']}\")\n",
					"tableNameList = ['FactInternetSales', 'FactResellerSales', 'DimCustomer', 'DimProduct', 'DimPromotion', 'DimDate']\n",
					"\n",
					"\"\"\"\n",
					"Iterate through the AdventureWorks tables in tableNameList. Each iteration will dynamically construct the table \n",
					"DDL to recreate and populate the new Delta table. The new Delta table will have the delta.enableChangeDataFeed \n",
					"set to 'true' and will add a new _Id identity column to make the synchronization process simpiler and faster\n",
					"for Synapse Dedicated SQL.\n",
					"\"\"\"\n",
					"for tableName in tableNameList:  \n",
					"    df = spark.read.format('parquet').load(f'{SynapseLakehouseSyncParameters[\"ParquetDataADLSFullPath\"]}/AdventureWorks_parquet/{tableName}') \n",
					"\n",
					"    # tableDDL = f\"CREATE TABLE {SynapseLakehouseSyncParameters['DatabaseName']}.{tableName} (_Id BIGINT GENERATED ALWAYS AS IDENTITY (START WITH 1 INCREMENT BY 1), \"\n",
					"    tableDDL = f\"CREATE TABLE {SynapseLakehouseSyncParameters['DatabaseName']}.{tableName} (\"\n",
					"    columnDDL = \"\"\n",
					"    \n",
					"    for field in df.schema.fields:\n",
					"        tableDDL += field.name + \" \" + str(field.dataType.typeName()) + ','\n",
					"        columnDDL += field.name + ', '\n",
					"\n",
					"    tableDDL = tableDDL[:-1] + f\") USING DELTA LOCATION '{SynapseLakehouseSyncParameters['ParquetDataADLSFullPath']}/AdventureWorks/{tableName}' TBLPROPERTIES (delta.enableChangeDataFeed = true)\"\n",
					"    spark.sql(tableDDL)\n",
					"\n",
					"    df = spark.read.format('parquet').load(f'{SynapseLakehouseSyncParameters[\"ParquetDataADLSFullPath\"]}/AdventureWorks_parquet/{tableName}').createOrReplaceTempView('vwBase')\n",
					"\n",
					"    spark.sql(f\"INSERT INTO {SynapseLakehouseSyncParameters['DatabaseName']}.{tableName} ({columnDDL[:-2]}) SELECT * FROM vwBase\")"
				],
				"execution_count": 4
			},
			{
				"cell_type": "code",
				"metadata": {
					"application/vnd.databricks.v1+cell": {
						"showTitle": false,
						"nuid": "effbca48-b9b1-4d11-b655-4eb7fa1ae44a",
						"title": ""
					}
				},
				"source": [
					""
				]
			}
		]
	}
}