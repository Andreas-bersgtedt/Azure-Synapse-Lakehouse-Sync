{
	"name": "Synapse Lakehouse Sync Tracking Table Log Success",
	"properties": {
		"folder": {
			"name": "Synapse Lakehouse Sync"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "SyncCluster",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 1,
			"runAsWorkspaceSystemIdentity": true,
			"conf": {
				"spark.dynamicAllocation.enabled": "true",
				"spark.dynamicAllocation.minExecutors": "1",
				"spark.dynamicAllocation.maxExecutors": "4",
				"spark.autotune.trackingId": "9c301000-ec07-43a4-bc38-e8d0c5fe93e4"
			}
		},
		"metadata": {
			"saveOutput": true,
			"synapse_widget": {
				"version": "0.1"
			},
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "python"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/18bc4af3-8099-4e70-86d8-eba06dd5bac8/resourceGroups/Azure-Synapse-Lakehouse-Sync/providers/Microsoft.Synapse/workspaces/synapsesyncqbq/bigDataPools/SyncCluster",
				"name": "SyncCluster",
				"type": "Spark",
				"endpoint": "https://synapsesyncqbq.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SyncCluster",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net",
					"authHeader": null
				},
				"sparkVersion": "3.3",
				"nodeCount": 5,
				"cores": 4,
				"memory": 28,
				"extraHeader": null
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"application/vnd.databricks.v1+cell": {
						"showTitle": false,
						"nuid": "6c05f6f4-604c-4ed0-aeae-8703f08306df",
						"title": ""
					}
				},
				"source": [
					"### Synapse Lakehouse Sync Tracking Table Log Success\n",
					"\n",
					"**This notebook calls the ClosedEntry function that updates the log entry record that the sync was a sucess in the _SynapseLakehouseTracking table.**\n",
					"\n",
					"#### Steps\n",
					"1. Import the Synapse Lakehouse Sync Functions\n",
					"2. Standardize the ADLS path provided\n",
					"3. Instantiant the Tracker class\n",
					"4. Call the CloseEntry function to update the Tracker record that the sync was successful\n",
					"\n",
					"#### Notebook Parameters\n",
					"- **PoolName** - The Synapse Dedicated Pool name that the data will be loaded into\n",
					"- **SynapseLakehouseSyncKey** - A pipe delimited hash (md5) of the pool name, schema name, table name, and row insert date time\n",
					"- **SynapseWorkspaceName** - The Synapse workspace name\n",
					"- **SyncFolderPathFull** - The full path in either abfss or https format that the CDC changes will be loaded to as well as where the _SynapseLakehouseSync delta table is stored\n",
					"- **TableRowCountSynapse** - The row count in the Synapse Dedicated Pool table"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"tags": [
						"parameters"
					]
				},
				"source": [
					"SynapseLakehouseSyncParameters = '''\r\n",
					"{\r\n",
					"    \"PoolName\": \"DataWarehouse\",\r\n",
					"    \"SynapseWorkspaceName\": \"synapsesyncqbq\",\r\n",
					"    \"SynapseLakehouseSyncKey\": \"cdae7bfac3427a0029b86bada994290d\",\r\n",
					"    \"TableRowCountSynapse\": 606,\r\n",
					"    \"SynapseSyncDataADLSFullPath\": \"abfss://gold@enterprisedatalakeqbq.dfs.core.windows.net/Sample/\"\r\n",
					"}\r\n",
					"'''"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"application/vnd.databricks.v1+cell": {
						"showTitle": true,
						"nuid": "d927c003-22b4-4a94-a292-2b8a6343828a",
						"title": "Parameters"
					},
					"tags": []
				},
				"source": [
					"import json\n",
					"\n",
					"SynapseLakehouseSyncParameters = json.loads(SynapseLakehouseSyncParameters)\n",
					"\n",
					"print(json.dumps(SynapseLakehouseSyncParameters, indent=4))"
				],
				"execution_count": 0
			},
			{
				"cell_type": "code",
				"metadata": {
					"application/vnd.databricks.v1+cell": {
						"showTitle": true,
						"nuid": "a930cf13-d390-415d-916d-7f411844c56a",
						"title": "Import Functions"
					}
				},
				"source": [
					"%run \"/Synapse Lakehouse Sync/Synapse Lakehouse Sync Functions\""
				],
				"execution_count": 0
			},
			{
				"cell_type": "code",
				"metadata": {
					"application/vnd.databricks.v1+cell": {
						"showTitle": true,
						"nuid": "870f0b93-0d7c-48ee-92a3-866a0c963d81",
						"title": "Standardize Folder Paths"
					}
				},
				"source": [
					"SyncFolderPathDict = UnifyFolderPaths('SynapseSync', f'{SynapseLakehouseSyncParameters[\"SynapseSyncDataADLSFullPath\"].rstrip(\"/\")}')\n",
					"\n",
					"print(SyncFolderPathDict)"
				],
				"execution_count": 0
			},
			{
				"cell_type": "code",
				"metadata": {
					"application/vnd.databricks.v1+cell": {
						"showTitle": true,
						"nuid": "e91a1cf8-cd30-486b-85b4-055e406f8aaf",
						"title": "Update the tracker table with success"
					}
				},
				"source": [
					"import time\n",
					"\n",
					"track = Tracker(SynapseWorkspaceName=SynapseLakehouseSyncParameters['SynapseWorkspaceName'], PoolName=SynapseLakehouseSyncParameters['PoolName'], TrackerFolderPath=SyncFolderPathDict[\"SynapseSync_abfss\"])\n",
					"\n",
					"# Due to the multiple updates to the delta tracking table, we will attempt 3 times waiting 5 seconds between attempts\n",
					"for _ in range(3):\n",
					"    try:\n",
					"        track.CloseEntry(SynapseLakehouseSyncParameters['SynapseLakehouseSyncKey'], SynapseLakehouseSyncParameters['TableRowCountSynapse'])\n",
					"    except Exception as e:\n",
					"        print(f'Error updating entry: {e}')\n",
					"        time.sleep(5) #wait for 5 seconds to try again\n",
					"        "
				],
				"execution_count": 0
			}
		]
	}
}